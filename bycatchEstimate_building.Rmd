---
title: "bycatchEstimate"
output: html_document
date: "2024-11-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, here, rstan)
```

Making some fake data 

Questions:
- How many subsamples do observers take? How large are those baskets/how many herring+bycatch should I expect to be in there? 
They aim for 10 samples, but can be higher or lower

https://www.nafo.int/Portals/0/PDFs/fc/proc/USA_2021ObserverOperationsManual.pdf

based on observer operations manual from 2021, observers record the weight of each species in each basket and the baskets' total weight

Thoughts after first pass: 
- Raw data is in pounds to nearest tenth, not numbers
- Probability/proportion RHS needs to be zero inflated
- I'm not sure it will even run with dependent values this high. 

Trying round (without zero inflation) to see

- The model still fits with fake data. 

```{r}
set.seed(29)
V<-20

prob_rhs_total<-0.01
prob_sigma<-0.01

alpha_fun<-function(mu, sigma){
  (((1-mu)/sigma^2)-(1/mu))*mu^2
}

beta_fun<-function(alpha, mu, sigma){
  alpha*((1/mu)-1)
}

alpha<-alpha_fun(prob_rhs_total, prob_sigma)
beta<-beta_fun(alpha, prob_rhs_total, prob_sigma)

prob_rhs_trip<-data.frame(VV=1:V, prob_rhs_trip = rbeta(V, alpha, beta))

total_catch_lbs<-round(rnorm(V, mean=50000, sd=10000), 1)
# abusing the binomial distribution, calling tenth of a pound estimates an integer..
total_catch_10th_lbs<-total_catch_lbs*10

# as a fake example, we have 20 observed trips, each with between 8 and 15 bucket samples
fake.df<-data.frame(VV=c(1:V),
                    NB=sample(c(8:15), 20, replace=T),
                    total_catch_10th_lbs=total_catch_10th_lbs)%>%
  mutate(expand=NB)%>%
  # expand rows by NB
  uncount(expand)%>%
  group_by(VV)%>%
  mutate(BB=row_number())%>%
  ungroup()%>%
  left_join(prob_rhs_trip, by="VV")%>%
  mutate(trip_alpha=alpha_fun(prob_rhs_trip, 0.01),
         trip_beta=beta_fun(trip_alpha, prob_rhs_trip, 0.01))%>%
  rowwise()%>%
  # the problem with this approach is that it doesn't include zero RHS trips. Really want a zero-inflated beta
  mutate(prop_RHS=rbeta(n=1, trip_alpha, trip_beta),
         lbs_10th_RHS=round(total_catch_10th_lbs*prop_RHS))
  # sum for binomial distribution (rather than bernoulli)
  # leave out baskets for now until I get simpler model running



data.list<-list(N=nrow(fake.df),
                V=max(fake.df$VV),
                #B=length(unique(paste(fake.df$VV, fake.df$BB, sep="_"))),
                VV=fake.df$VV,
               # NB=fake.df$NB,
              #  BB=fake.df$BB,
              # nFish is the number of "trials" for the binomial draws
                totalCatch=fake.df$total_catch_10th_lbs,
                y=fake.df$lbs_10th_RHS
                )
```

Fit model

Troubleshooting options: 
- informative prior: tried this an dhad same problem. log probability at initial value is out of range. (example: Rejecting initial value:
Chain 1:   Error evaluating the log probability at the initial value.
Chain 1: Exception: beta_lpdf: Random variable is 1.57918, but must be in the interval [0, 1] (in 'string', line 36, column 2 to column 29))

- try higher (simulated) probability of RHS, maybe it's having trouble estimating a small value
this didn't work

- set initial values?
Why was initialization between -2 and 2? 

- separating step of estimating alpha and beta parameters? (see handbook)

- reduced model complexity--observers now draw straight from trip catch rather than bucket

- interestingly, adding upper bound on p_rhs_trip largely addressed initialization problem. There were a couple of rejected initial values and a warning error, I think about model run time? Note that this run was successful. Still had big problem with divergent transitions, chain mixing, low ESS

- after adding kappa (and switching from bernoulli to binomial), model now runs but has huge divergent transitions (every iteration!) Increasing the simulated probability of RHS doesn't help. Reducing step size also did nothing. 

helpful source suggests alternative parameterization in terms of log-odds alpha

```{r}
fit<-stan(file="reducedBycatchModel_logit.stan",
          data=data.list,
          warmup=1000,
          iter=3000,
          chains=4,
          cores=4,
          control=list(stepsize=0.01, adapt_delta=0.99))

post<-extract(fit)

summary(fit)

hist(post$prob_rhs_total)
```
That's actually not quite right. This might be a problem with how I generated the data
oh, I think it's the influence of bucket probabilities

Real data will probably need non-centered parameterization
