---
title: "bycatchEstimate"
output: html_document
date: "2024-11-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, here, rstan)
```

Making some fake data 

Questions:
- How many subsamples do observers take? How large are those baskets/how many herring+bycatch should I expect to be in there? 

```{r}
set.seed(29)
V<-20

prob_rhs_total<-0.01
prob_rhs_trip<-data.frame(VV=1:V, prob_rhs_trip = rbeta(V, V*prob_rhs_total, V*(1-prob_rhs_total)))
# this will have to be a function


# as a fake example, we have 20 observed trips, each with between 1 and 10 bucket samples
fake.df<-data.frame(VV=c(1:V),
                    NB=sample(c(1:10), 20, replace=T))%>%
  mutate(expand=NB)%>%
  # expand rows by NB
  uncount(expand)%>%
  group_by(VV)%>%
  mutate(BB=row_number())%>%
  ungroup()%>%
  left_join(prob_rhs_trip, by="VV")%>%
  rowwise()%>%
  mutate(prob_rhs_bucket=rbeta(1, NB*prob_rhs_trip, NB*(1-prob_rhs_trip)),
         nFish=sample(c(50:75), 1, replace=TRUE))%>%
  ungroup()%>%
  # now add nfish sampled and expand for bernoulli draws
  # I don't know the size of thes baskets; for now I'll assume there are 50-75 fish in each one
  uncount(nFish)%>%
  rowwise()%>%
  mutate(RHS=rbinom(n=1, size=1, prob=prob_rhs_bucket))%>%
  ungroup()%>%
  # sum for binomial distribution (rather than bernoulli)
  # leave out baskets for now until I get simpler model running
  group_by(VV, BB)%>%
  summarize(nFish=n(),
            nRHS=sum(RHS))


data.list<-list(N=nrow(fake.df),
                V=max(fake.df$VV),
                #B=length(unique(paste(fake.df$VV, fake.df$BB, sep="_"))),
                VV=fake.df$VV,
               # NB=fake.df$NB,
              #  BB=fake.df$BB,
              # nFish is the number of "trials" for the binomial draws
                nFish=fake.df$nFish,
                y=fake.df$nRHS
                )
```

Fit model

Troubleshooting options: 
- informative prior: tried this an dhad same problem. log probability at initial value is out of range. (example: Rejecting initial value:
Chain 1:   Error evaluating the log probability at the initial value.
Chain 1: Exception: beta_lpdf: Random variable is 1.57918, but must be in the interval [0, 1] (in 'string', line 36, column 2 to column 29))

- try higher (simulated) probability of RHS, maybe it's having trouble estimating a small value
this didn't work

- set initial values?
Why was initialization between -2 and 2? 

- separating step of estimating alpha and beta parameters? (see handbook)

- reduced model complexity--observers now draw straight from trip catch rather than bucket

- interestingly, adding upper bound on p_rhs_trip largely addressed initialization problem. There were a couple of rejected initial values and a warning error, I think about model run time? Note that this run was successful. Still had big problem with divergent transitions, chain mixing, low ESS

- after adding kappa (and switching from bernoulli to binomial), model now runs but has huge divergent transitions (every iteration!) Increasing the simulated probability of RHS doesn't help. Reducing step size also did nothing. 

helpful source suggests alternative parameterization in terms of log-odds alpha

```{r}
fit<-stan(file="reducedBycatchModel_logit.stan",
          data=data.list,
          warmup=1000,
          iter=3000,
          chains=4,
          cores=4,
          control=list(stepsize=0.01, adapt_delta=0.99))

post<-extract(fit)

summary(fit)

hist(post$prob_rhs_total)
```
That's actually not quite right. This might be a problem with how I generated the data
oh, I think it's the influence of bucket probabilities

Real data will probably need non-centered parameterization
